\documentclass[8pt,landscape]{extarticle}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage[hidelinks]{hyperref}
\usepackage{tabularx}
\newcommand\NumCols{3}
\pdfinfo{
  /Title (formula-sheet-2.pdf)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (Garrett Weaver)
  /Subject (SIE 530)
  /Keywords (statistics, engineering, masters, Arizona, Liu)}

% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
    {\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
    }

% Turn off header and footer
\pagestyle{empty}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

%My Environments
\newtheorem{example}[section]{Example}
% -----------------------------------------------------------------------

\begin{document}

\raggedright
%\footnotesize\usepackage{amsmath}
\begin{multicols}{\NumCols}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\underline{SIE 530 Formula Sheet 2}} \\
\end{center}

\section*{Snedecor's \emph{F}-Distribution}
Consider these two distributions \\
$X_1, \dots X_n \sim N(\mu_X, \sigma_X^2)$ \\
$Y_1, \dots Y_m \sim N(\mu_Y, \sigma_Y^2)$ \\
RV $F$ is the ratio of two independent $\chi^2$ variables divided by their respective degree of freedom:
$$F=(S_X^2/\sigma_X^2)/(S_Y^2/\sigma_Y^2)$$
has Snedecor's $F$-Distribution with $v_1=n-1$ and $v_2=m-1$ degrees of freedom (i.e. $F \sim F_{v_1,v_2}$)

$$f_F(x|v)=\frac{\Gamma(\frac{v_1+v_2}{2})}{\Gamma(\frac{v_1}{2})\Gamma(\frac{v_2}{2})}
\begin{pmatrix} \frac{v_1}{v_2} \end{pmatrix}^{\frac{v_1}{2}}
\frac{x^{\frac{(v_1-2)}{2}}}{\begin{pmatrix}1+\begin{pmatrix}\frac{v_1}{v_2}\end{pmatrix}x\end{pmatrix}^{\frac{(v_1+v_2)}{2}}}$$
$0\leq\infty, v_1,v_2=1\dots$

\subsection*{$F$-Distribution Properties}
\begin{itemize}
\item $EX=\frac{v_2}{v_2-2}, v_2 > 2$
\item $VarX = 2(\frac{v_2}{v_2-2})^2\frac{v_1+v_2-2}{v_1(v_2-4)}, v_2>4$
\item If $X\sim F_{v_1,v_2}$ then $1/X \sim F_{v_2,v_1}$
\item If $X \sim t_q$ then $X^2 \sim F_{1,q}$
\item If $X \sim F{v_1,v_2}$ then \\ $\frac{(v_1/v_2)X}{(1+(v_1/v_2)X)}\sim beta(\frac{v_1}{2},\frac{v_2}{2})$
\end{itemize}

\section*{Sufficient Statistics}
Contains all information about the parameters. 
A statistic $T(X)$ is sufficient for $\theta$ if the conditional distribution of the sample $X$ given the value of $T(X)$ does not depend on $\theta$.

\subsection*{Theorem}
If $p(x|\theta)$ is the joint pdf or pmf of $X$ and $q(T(x)|\theta)$ is the pdf or pmf of $T(X)$ then $T(X)$ is a sufficient statistic of $\theta$ if, for every $x$ in the sample space the ratio
$$\frac{p(x|\theta)}{q(T(x)|\theta)}$$
is constant as a function of $\theta$.
\begin{enumerate}
\item $P(x|p)=\prod\limits_{i=1}^nP(X=x)$
\item Find $q(T(x)|\mu)$
\item Find $\frac{p(x|\theta)}{q(T(x)|\theta)}$
\end{enumerate}

\subsection*{Factorization Theorem}
A statistic $T(X)$ is sufficient statistic for $\theta$ \emph{if and only if} there exist function $g(T(x)|\theta)$ and $h(x)$ such that, for all sample points $x$ and all parameter point $\theta$,
$$f(x|\theta)=g(T(x)|\theta) \cdot h(x)$$

\section*{Complete Statistic}
Contains no irrelevant information about the parameters.
Let $f(T(x)|\theta)$ be a family of pdfs or pmfs for a statistic $T(X)$. The family of probability distributions is called \emph{complete} if $e_\theta g(T)=0$ for all $\theta$ implies $P_\theta(g(T)=0)=1$ for all $\theta$. Equivalently, $T(X)$ is called a \emph{Complete statistic}.

\section*{Likelihood function}
$f(x|\theta) \sim X = (X_1 \dots X_2)$ If $X=x$ is observed, the function of $\theta$ defined by
$$L(\theta|x)=f(x|\theta)$$
is a likelihood function. 

\subsection*{Likelihood principle}
Given points $x$ and $y$ $\exists C(x,y)$ where C is a constant such that
$$L(\theta|x)=C(x,y)L(\theta|y), \forall \theta$$

\section*{Estimators}

\subsection*{Method of Moment (MoM)}
$$\begin{cases}
\frac{1}{n}\sum\limits_{i=1}^{n}X_i = E(X^1_i) & First Moment \\
\frac{1}{n}\sum\limits_{i=1}^{n}X_i^2 = E(X^2_i) & Second Moment \\
\vdots \quad \vdots \quad \vdots & \vdots \\
\frac{1}{n}\sum\limits_{i=1}^{n}X_i^k = E(X^k_i) & k^{th} Moment \\
\end{cases}$$
You have n equations and n unknowns, solve for $\mu$, $\sigma$, etc.
\subsection*{Maximum Likelihood Estimation (MLE)}
\begin{itemize}
\item Write down likelihood function \\ $L(\theta|x)=\prod\limits_{i=1}^{n}P_{\theta}(X=x)$
\item Solve for theta\\ $\frac{\partial ln(L(\theta|x))}{\partial \theta_i}=0$
\item Verify the second derivative is less than zero\\ $\frac{\partial^2 ln(L(\theta|x))}{\partial \theta_i^2}<0$
\end{itemize}

\subsection*{Bayes Estimator}
$$\pi(\theta|x)=\frac{f(x|\theta)\pi(\theta)}{m(x)}$$
$$m(x)=\int f(x|\theta)\pi(\theta)d\theta$$

\subsection*{Evaluating Estimators}
\subsubsection*{Bias}
$$Bias(\hat{\theta})=E(\hat{\theta})-\theta$$

\begin{itemize}
\item Measures the \emph{accuracy} of the estimator
\item An estimator whose bias is zero is called \emph{unbiased}
\item An unbiased estimator may, nevertheless, fluctuate greatly from sample to sample
\end{itemize}

\subsubsection*{Var}
$$Var(\hat{\theta})=E{[\hat{\theta}-E(\hat{\theta})]^2}$$

\begin{itemize}
\item The lower the vaiance, the more \emph{precise} the estimator.
\item A low-variance estimator may be biased
\item Among the unbiased estimators, the one with the lowest variance should be chosen. "Best" - minimum variance
\end{itemize}

\subsubsection*{Mean Squared Error}
\begin{itemize}
\item To choose among all estimators (biased and unboased). Minimze a measure that combines both bias and variance.
\item A "good" estimator should have low bias and (accurate) AND low vairance (precise).
\end{itemize}
$$MSE(\hat{\theta})=E{[\hat{\theta}-\theta]^2}=Var(\hat{\theta})+[Bias(\hat{\theta})]^2$$
$$Bias(\hat{\theta})=E(\hat{\theta})-\theta$$
$$Var(\hat{\theta})=E{[\hat{\theta}-E(\hat{\theta})]^2}$$


\subsection*{Uniform Minimum Variance Unbiased Estimator (UMVUE)}
An estimator $W*$ is a \emph{best unbiased} estimator of $\tau(\theta)$ if it statisfies $E_\theta W* = \tau(\theta), \forall \theta$ and for any other estimator $W$ with $E_\theta W = \tau(\theta)$ we have $Var_\theta W* \leq Var_\theta W$

\subsection*{Relating Sufficiency Stats to Unbiased Est.}
Given X and Y as RVs and the expectations exist, then
$$E(X)=E[E(X|Y)]$$
$$Var(X) = Var[E(X|Y)]+E[Var(X|Y)]$$

\subsection*{Rao-Blackwell theorem}
$sum_{k=0}^\infty h(x) *g(x)$ where $g(x)$ is the distribution and $h(x)$ is some function. 
If you show that the function is zero only when $h(x)$ is zero then it passes the RB theorem. 

If you have complete, sufficient and unbiased and if the distribution passes this theorem then it is a UMVUE. 

\section{Hypothesis Testing}

\subsection{Likelihood Ratio Test (LRT)}
Testing $H_0:\theta \in \Theta_0$ vs $H_1:\theta \in \Theta_0^C$ is 
$$\lambda(x)=\frac{sup_{\Theta_0} L(\theta|x)}{sup_{\Theta} L(\theta|x)}$$
Top part is the ratio between 2 maximum likelihood tests $\Theta=\Theta_0 \cup \Theta_0^C$ Bottom part is the global max. 
A LRT is any test that has a rejection region of the form $\{x:\lambda(x)\leq c\}$ where c is an number satisfying $0 \leq c \leq 1$.

\subsection{Inference on MEAN of normal with Variance Known}
Use Z test
$$\frac{\bar{x}-\mu_0}{\sigma / \sqrt{n}} \sim N(0,1); \quad \bar{x}-Z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{x} + Z_{\alpha/2}\frac{\sigma}{\sqrt{2}}$$
\begin{tabular}{| c | c | c |}
\hline
$H_0:\mu=\mu_0$ & $H_1$ & \\ 
Reject $H_0$: & $\mu \neq \mu_0$ & if $\begin{vmatrix} \frac{\bar{x}-\mu_0}{\sigma / \sqrt{n}} \end{vmatrix} > Z(\alpha / 2)$  \\ \hline
Reject $H_0$: & $\mu < \mu_0$ & if $\frac{\bar{x}-\mu_0}{\sigma / \sqrt{n}} < -Z(\alpha)$  \\ \hline
Reject $H_0$: & $\mu > \mu_0$ & if $\frac{\bar{x}-\mu_0}{\sigma / \sqrt{n}} > Z(\alpha)$  \\ \hline
\end {tabular}

\subsection{Inference on MEAN of normal with Variance Unknown}
Use t test
$$\frac{\bar{x}-\mu}{s / \sqrt{n}} \sim t_{(n-1)}; \quad \bar{x}-t_{\alpha/2} \frac{s}{\sqrt{n}} \leq \mu \leq \bar{x} + t_{\alpha/2}\frac{s}{\sqrt{n}}$$
\begin{tabular}{| c | c | c |}
\hline
$H_0:\mu=\mu_0$ & $H_1$ & \\ 
Reject $H_0$: & $\mu \neq \mu_0$ & if $\begin{vmatrix} \frac{\bar{x}-\mu_0}{s / \sqrt{n}} \end{vmatrix} > t(\alpha / 2, n-1)$  \\ \hline
Reject $H_0$: & $\mu < \mu_0$ & if $\frac{\bar{x}-\mu_0}{s / \sqrt{n}} < -t(\alpha, n-1)$  \\ \hline
Reject $H_0$: & $\mu > \mu_0$ & if $\frac{\bar{x}-\mu_0}{s / \sqrt{n}} > t(\alpha, n-1)$  \\ \hline
\end {tabular}

\subsection{Significance Testing}
Significance is the area underneath the normal curve up to a certain point on the x axis. When testing for significance you want to find the point on the x axis, ($Z$), which equals the value of the significance from the standard normal CDF. When this $Z$ value is found then it can be compared with a calculated sigma calculation: $\sigma = \sqrt{npq}$ where $n$ is the sample size, $p$ is the claimed probability $q = 1-p$. 


Remember\\
$E(\bar{x})=\mu$\\
$Var(\bar{x})=\frac{\sigma^2}{n}$\\
$E(S^2)=\sigma^2$\\
$Var(S^2)=\frac{2 \sigma^4}{n-1}$\\
$\sigma^2 = \frac{1}{n} \sum (x_i-\bar{x})^2 = \frac{(n-1)s^2}{n}$\\

Remeber, when sampling with the distribution on the left you get the distribution on the right\\
Normal -> Normal\\
Bernolli -> Binomial\\
Exponential -> Gamma\\

% Add in the point estimation slide 115 from lecture 15 
% Normal, Gamma, Binomial, Poisson 

\scriptsize
\bibliographystyle{unsrt} 
\bibliography{course-references}
\end{multicols}
\end{document}

