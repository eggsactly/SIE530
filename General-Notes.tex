\documentclass[12pt]{article}
\usepackage{lingmacros}
\usepackage{tree-dvips}
\usepackage{amsmath}
\usepackage{accents}
\usepackage[hidelinks]{hyperref}
\usepackage{tabularx} % in the preamble

\newcommand{\ubar}[1]{\underaccent{\bar}{#1}}

\setlength\parindent{0pt}

\begin{document}

\title{SIE 530 Comprehensive Notes}
\date{October 1, 2017\\Version: 2}
\maketitle
{\centering

Fork on GitHub: \url{https://github.com/eggsactly/SIE530}\par
}
\section*{Definitions}

\begin{itemize}

\item Disjoint: Event $A$ and event $B$ are disjoint, or mutually exclusive, if $A$ and $B$ have no outcome in common $A \cap B = \emptyset \leftrightarrow$ $A$ and $B$ are disjoint events. \cite[p.18]{classnotes.1}

\item Event: A collection of elements contained in the \emph{sample space} \cite[p.16]{classnotes.1}

\item Exhaustive: Event $A$ and event $B$ are exhaustive if the union is $S$. If $A \cap B = \emptyset \leftrightarrow$ $A$ and $B$ are exhaustive events. \cite[p.18]{classnotes.1}

\item Independent: Two events $A$ and $B$ are said to be independent if $P(A|B)=P(A)$ \cite[p.8]{classnotes.3}

\item Median: A value such that at least 50\% of the data values are at of below this value and at least 50\% of the data values are at or above this value. (Not sensitive to the outlier). \cite[p.9]{classnotes.1}

\item Nondecreasing: A function f(x) is said to be \emph{nondecreasing} on an interval $I$ if $f(b)\geq f(a)$ for all $b>a$, where $a,b \in I$. \cite{wolfram.nondecreasing}

\item Population: A finite well defined group, which can be enumerated in theory. \cite[p.9]{classnotes.1}

\item Random Experiment: an action or process whos outcome is uncertain \cite[p.16]{classnotes.1}

\item Random Variable: A variable with unknown numerical value that can take on or represent any possible element from a \emph{sample space} \cite[p.10]{classnotes.3}

\item Right Continuous: A function is \emph{Right Continuous} when a point as approached from the right \cite[p.13]{classnotes.3}

\item Sample: A subset of a population obtained through a process (possibly random selection). \cite[p.9]{classnotes.1}

\item Sample Space ($S$): A collection of all possible outcomes of a \emph{Random Experiment} \cite[p.16]{classnotes.1}

\end{itemize}

\newpage

\section*{Probability Theory}

Central Tendency: sample average/mean \cite[p.14]{classnotes.1}.
$$\bar{x}=\frac{\sum\limits_{i=1}^{n} x_i}{n}$$

Scatter/dispersion: \emph{sample variance} or \emph{sample standard deviation} \cite[p.14]{classnotes.1}.

$$\hat{\sigma}=S=\sqrt{\frac{\sum\limits_{i=1}^{n}(x_i-\bar{x})^2}{n-1}}$$

\subsection*{Set Theory}
\begin{itemize}

\item $s\in S$: The outcome $s$ belongs to the sampel space $S$ \cite[p.17]{classnotes.1}

\item $\emptyset=\{\}$: The empty set, the set of no elements. Defines the set of elements of an impossible event \cite[p.17]{classnotes.1}

\item Union: The union of event $A$ and event $B$ denonted $A \cup B$, is the collection (or set) of elements that belong to \emph{either} $A$ or $B$ or both \cite[p.17]{classnotes.1}

\item Intersection: The intersection of event $A$ and event $B$ denoted $A \cap B$ is the collection (or set) of elements that belong to $A$ and $B$ \cite[p.17]{classnotes.1}

\item Complementation: collection of elements that do not belong to $A$. $A^c=\{x:x\notin A\}$ \cite[p.17]{classnotes.1}

\item $A\subset B$: The event course-references.bib$A$ is contained in event $B$, $A\subset B$, if every element of $A$ also belonegs to $B$ \cite[p.17]{classnotes.1}

\end{itemize}

\subsection*{Properties of Sets \cite[p.19]{classnotes.1}}

\begin{itemize}

\item Commutativity: $A \cup B = B \cup A; A \cap B = B \cap A$

\item Associativity: $A\cup (B \cup C)=(A \cup B) \cup C; A \cap (B \cap C) = (A \cap B) \cap C$

\item Distributive Laws: $A \cap(B \cup C) = (A \cap B) \cup ( A \cap C)$ $ A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$

\item DeMorgan's Laws:
$(A \cup B)^c=A^c \cap B^c$
$(A \cap B)^c=A^c\cup B^c$

\end{itemize}

\subsection*{Definition of Probability \cite[p.4]{classnotes.2}}
[P \(\) : \{set of all possible events\} $\rightarrow [0,1]$]
\begin{itemize}

\item Axiom 1: For any event $A, P(A)\geq 0$ (nonnegatove).
\item Axiom 2: $P(S)=1$
\item Axiom 3: For any sequence of disjoint sets $A_1,A_2,\dots,A_n, P(A_1 \cup A_2 \cup \dots A_n)=\sum\limits_{i=1}^nP(A_i)$, where n is the total number of disjoint sets in the sequence.

\end {itemize}

\subsection*{Properties and Additive Laws \cite[p.5]{classnotes.2}}
\begin{itemize}
\item $P(A)=1-P(A^c);$
\item $P(\emptyset)=0$
\item $P(S)=1$
\item if $A$ and $B$ are disjoint $P(A\cup B)=0$
\item $A\subset B \rightarrow P(A)\leq (B)$
\item $P(A \sup B) = P(A) + P(B) - P(A \cap B)$
\end{itemize}

Bonferroni's inequality \cite[p.6]{classnotes.2}
$$P(A\cap B) \geq P(A) + P(B) - 1$$

Boole's inequality \cite[p.5]{classnotes.2}: for any sets $A_1, A_2, \dots \subset S$
$$P(\bigcup\limits_{i=1}^\infty)\leq \sum\limits_{i=1}^\infty P(A_i)$$

\subsection*{Counting Techniquies}

\begin{itemize}
\item Permutations: Select k objects from a total n without replacement and order matters, what is the number of permutations? \cite[p.9]{classnotes.2}:
$$P_{k,n} = \frac{n!}{(n-k)!}$$
\item Combinations: Select k objects from a total of n without replacement and order does not matter, what is the number of combinations? \cite[p.10]{classnotes.2}
$$C_{k,n}=
\begin{pmatrix}
n \\
k
\end{pmatrix}
=\frac{n!}{k!(n-k)!}
$$
\end{itemize}

\subsection*{Conditional Probability}
Probability of $A$ such that $B$ has occured \cite[p.12]{classnotes.2}:
$$P(A|B)=\frac{P(A \cap B)}{P(B)}$$ 
$$P(A|B)P(B)=P(B|A)P(A)$$
$$P(A_1|B)+P(A_2|B)+\dots P(A_k|B)=1$$
Law of total probability:
$$\sum\limits_{i=1}^k P(B|A_i)P(A_i)=P(B)$$

\subsection*{Bayes Theorem}
Let the events $A_1,A_2,\dots A_k$ be \emph{disjoint} and \emph{exhaustive} events in the sample space $S$, such that $P(A_i)>0$ and let $B$ be an event such that $P(B)>0$, then, \cite[p.7]{classnotes.3}:
$$P(A_i|B)=\frac{P(B|A_i)P(A_i)}{\sum_{j=1}^k P(B|A_i)P(A_j)}$$
$$P(A_i|B)=\frac{P(A_i \cap B)}{P(B)} = \frac{cond. \quad prob.}{law \quad of \quad total \quad prob.}$$

\subsection*{Discrete Random Variables}
\emph{Probability mass function} (pmf) of a discrete \emph{Random Variable} is given by \cite[p.11]{classnotes.3}:
$$f_X(x)=P(X=x)$$
and satisfies the following properties
\begin{enumerate}
\item $f_X(x)\geq 0; \quad \forall x$
\item $\sum\limits_{i=1}^{\infty}f(x_i)=1$
\end{enumerate}

\emph{Cumulative Distribution Function} (cdf) of a discrete \emph{Random Variable} \cite[p.13]{classnotes.3}:
$$F_X(x)=P(X\leq x)=\sum\limits_{i=1}^{x}f_X(i); \quad \forall x$$

Satisfies the following properties
\begin{enumerate}
\item $\lim\limits_{x\to-\infty}F(x)=0$ and $\lim\limits_{x\to\infty}F(x)=1$
\item $F(x)$ is a \emph{nondecreasing} function of $x$
\item $F(x)$ is \emph{right-continuous} for every number $x_0, \quad \lim_{x\downarrow x_0}F(x)=F(x_0)$
\end{enumerate}

\subsection*{Continuous Random Variables}
The \emph{probability density function} (pdf) of a continuous \emph{Random Variable} is given by \cite[p.4]{classnotes.4}:
$$\int_{x_1}^{x_2}f_X(x)dx=P(x_1\leq X\leq x_2), \quad \forall x_1,x_2$$
And satisfies the following properties:
\begin{enumerate}
\item $f(x)\geq 0 \quad \forall x$
\item $\int_{-\infty}^{\infty}f(x)dx=1$
\end{enumerate}

The \emph{Cumulative Distribution Function}(cdf) for a continuous \emph{Random Variable} is \cite[p.5]{classnotes.4}:
$$F_X(x)=P(X \leq x) = \int_{-\infty}^{x}f_X(u)du, \quad \forall x$$
And satisifes the following property:
\begin{enumerate}
\item $\frac{\partial F_X(x)}{\partial x}=f_X(x)$ provided that $F'$ exists and $X$ is a continuous \emph{Random Variable}
\end{enumerate}

\section*{Table of Common Distributions}
\subsection*{Discrete Distributions}
\subsubsection*{Bernoulli(p) \cite[p. 621]{StatisticalInference}}
\begin{tabularx}{\textwidth}{ l X }
\emph{pmf} & $P(X=x|p)=p^x(1-p)^{1-x}; \quad x=0, 1; \quad 0 \leq p \leq 1$ \\
\emph{mean} & $EX=p$ \\
\emph{variance} & $Var X =p(1-p)$ \\
\emph{mgf} & $M_X(t)=(1-p)+pe^t$ \\
\emph{Notes} & None\\
\end{tabularx}

\subsubsection*{Binomial(n, p) \cite[p. 621]{StatisticalInference}}
\begin{tabularx}{\textwidth}{ l X }
\emph{pmf} & $P(X=x|n, p)=\begin{pmatrix}
n \\
x
\end{pmatrix}p^x(1-p)^{n-x}; \quad x=0, 1, 2, \dots, n; \quad 0 \leq p \leq 1$ \\
\emph{mean} & $EX=np$ \\
\emph{variance} & $Var X =np(1-p)$ \\
\emph{mgf} & $M_X(t)=[pe^t + (1-p)]^n$ \\
\emph{Notes} & None\\
\end{tabularx}

\subsubsection*{Hypergeometric \cite[p. 622]{StatisticalInference}}
\begin{tabularx}{\textwidth}{ l X }
\emph{pmf} & $P(X=x|N, M, K)=\frac{
\begin{pmatrix}
M \\
x
\end{pmatrix}
\begin{pmatrix}
N - M \\
k - x
\end{pmatrix}
}{
\begin{pmatrix}
N \\
K
\end{pmatrix}}; \quad x=0, 1, 2, \dots, K; \quad M - (N - K) \leq x \leq M; \quad N, M, K \geq 0$ \\
\emph{mean} & $EX=\frac{KM}{N}$ \\
\emph{variance} & $Var X = \frac{KM}{N} \frac{(N-M)(N-K)}{N(N-1)}$ \\
\emph{mgf} & None \\
\emph{Notes} & If $K \ll M$ and $N$, the range $x=0, 1, 2, \dots, K$ will be appropriate.\\
\end{tabularx}

\subsubsection*{Poisson($\lambda$) \cite[p. 622]{StatisticalInference}}
\begin{tabularx}{\textwidth}{ l X }
\emph{pmf} & $P(X=x|\lambda)=\frac{e^{-\lambda}\lambda^x}{x!}; \quad x=0, 1, \dots; \quad 0 \leq p \leq \infty$ \\
\emph{mean} & $EX=\lambda$ \\
\emph{variance} & $Var X =\lambda$ \\
\emph{mgf} & $M_X(t)=e^{\lambda(e^t-1)}$ \\
\emph{Notes} & None\\
\end{tabularx}

\subsection*{Continuous Distributions}
\subsubsection*{Chi squared(p)\cite[p. 623]{StatisticalInference}}
\begin{tabularx}{\textwidth}{ l X }
\emph{pmf} & $f(x|p)=\frac{1}{\Gamma(p/2)2^{p/2}}x^{(p/2)-1}e^{-x/2}; \quad 0 \leq x \leq \infty; \quad p=1, 2, \dots$ \\
\emph{mean} & $EX = p$ \\
\emph{variance} & $Var X = 2p$ \\
\emph{mgf} & $M_X(t)=\begin{pmatrix} \frac{1}{1-2t} \end{pmatrix}^{p/2}, \quad t < \frac{1}{2}$ \\
\emph{Notes} & Special case of the gamma distribution.\\
\end{tabularx}

\subsubsection*{Exponential($\beta$)\cite[p. 624]{StatisticalInference}}
\begin{tabularx}{\textwidth}{ l X }
\emph{pmf} & $f(x|p)=\frac{1}{\beta}e^{-x/\beta}; \quad 0 \leq x \leq \infty; \quad \beta>0$ \\
\emph{mean} & $EX = \beta$ \\
\emph{variance} & $Var X = \beta^2$ \\
\emph{mgf} & $M_X(t)= \frac{1}{1-\beta t}, \quad t < \frac{1}{\beta}$ \\
\emph{Notes} & Special case of the gamma distribution. Has the \emph{memoryless} property. Has many special cases: $Y=X^{1/\gamma}$ is \emph{Weibull}, $Y=\sqrt{2X/\beta}$ is \emph{Rayleigh}, $Y=\alpha - \gamma log(X/\beta)$ \emph{Gumbel}\\
\end{tabularx}

\subsubsection*{Gamma($x|\alpha, \beta$)\cite[p. 624]{StatisticalInference}}
\begin{tabularx}{\textwidth}{ l X }
\emph{pmf} & $f(x|\alpha, \beta)=\frac{1}{\Gamma(\alpha)\beta^{\alpha}} x^{\alpha - 1}e^{-x/\beta}; \quad 0 \leq x < \infty; \quad \alpha\beta>0$ \\
\emph{mean} & $EX = \alpha\beta$ \\
\emph{variance} & $Var X = \alpha\beta^2$ \\
\emph{mgf} & $M_X(t)= \frac{1}{1-\beta t}^{\alpha}, \quad t < \frac{1}{\beta}$ \\
\emph{Notes} & Some special cases are exponential ($\alpha = 1$) and shi squared ($\alpha = p/2, \beta = 2$). If $\alpha = \frac{3}{2}$, $Y=\sqrt{X/\beta}$ is \emph{Maxwell}, $Y=1/X$ has the \emph{inverted gama distribution}. Can also be related to the Poisson.\\
\end{tabularx}

\subsubsection*{Normal($\mu, \sigma^2$)\cite[p. 625]{StatisticalInference}}
\begin{tabularx}{\textwidth}{ l X }
\emph{pmf} & $f(x|\mu, \sigma^2)=\frac{1}{\sqrt{2\pi \sigma}}e^{-(x-\mu)^2/(2\sigma^2)}; \quad -\infty < x < \infty; \quad -\infty < \mu < \infty$ \\
\emph{mean} & $EX = \mu$ \\
\emph{variance} & $Var X = \sigma^2$ \\
\emph{mgf} & $M_X(t)= e^{\mu t + \sigma^2t^2/2}$ \\
\emph{Notes} & Sometimes called the \emph{Gaussian} distribution.\\
\end{tabularx}

\subsubsection*{Uniform($a, b$)\cite[p. 626]{StatisticalInference}}
\begin{tabularx}{\textwidth}{ l X }
\emph{pmf} & $f(x|a, b)=\frac{1}{b-a}, \quad a \leq x \leq b$ \\
\emph{mean} & $EX = \frac{b+a}{2}$ \\
\emph{variance} & $Var X = \frac{(b-a)^2}{12}$ \\
\emph{mgf} & $M_X(t)=\frac{e^{bt}-e^{at}}{(b-a)t}$ \\
\emph{Notes} & If $a=0$ and $b=1$, this is a special case of the beta $(\alpha = \beta = 1)$.\\
\end{tabularx}

\newpage
\bibliographystyle{unsrt}
\bibliography{course-references}

\end{document}
