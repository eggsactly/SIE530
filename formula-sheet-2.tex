\documentclass[8pt,landscape]{extarticle}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage[hidelinks]{hyperref}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{alltt}

\newcommand\NumCols{3}
\pdfinfo{
  /Title (formula-sheet-2.pdf)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Subject (SIE 530)
  /Keywords (statistics, engineering, masters, Arizona, Liu)}

% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
    {\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
    }

% Turn off header and footer
\pagestyle{empty}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

%My Environments
\newtheorem{example}[section]{Example}
% -----------------------------------------------------------------------

\begin{document}

\raggedright
%\footnotesize\usepackage{amsmath}
\begin{multicols}{\NumCols}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\underline{SIE 530 Formula Sheet 2 - Fall 2017}} \\
\end{center}

\begin{alltt}
Source is at: \input{remote.txt}
\end{alltt}

\section*{Snedecor's \emph{F}-Distribution}
Consider these two distributions \\
$X_1, \dots X_n \sim N(\mu_X, \sigma_X^2)$ \\
$Y_1, \dots Y_m \sim N(\mu_Y, \sigma_Y^2)$ \\
RV $F$ is the ratio of two independent $\chi^2$ variables divided by their respective degree of freedom:
$$F=(S_X^2/\sigma_X^2)/(S_Y^2/\sigma_Y^2)$$
has Snedecor's $F$-Distribution with $v_1=n-1$ and $v_2=m-1$ degrees of freedom (i.e. $F \sim F_{v_1,v_2}$)

$$f_F(x|v)=\frac{\Gamma(\frac{v_1+v_2}{2})}{\Gamma(\frac{v_1}{2})\Gamma(\frac{v_2}{2})}
\begin{pmatrix} \frac{v_1}{v_2} \end{pmatrix}^{\frac{v_1}{2}}
\frac{x^{\frac{(v_1-2)}{2}}}{\begin{pmatrix}1+\begin{pmatrix}\frac{v_1}{v_2}\end{pmatrix}x\end{pmatrix}^{\frac{(v_1+v_2)}{2}}}$$
$0\leq\infty, v_1,v_2=1\dots$

\subsection*{$F$-Distribution Properties}
\begin{itemize}
\item $EX=\frac{v_2}{v_2-2}, v_2 > 2$
\item $VarX = 2(\frac{v_2}{v_2-2})^2\frac{v_1+v_2-2}{v_1(v_2-4)}, v_2>4$
\item If $X\sim F_{v_1,v_2}$ then $1/X \sim F_{v_2,v_1}$
\item If $X \sim t_q$ then $X^2 \sim F_{1,q}$
\item If $X \sim F{v_1,v_2}$ then \\ $\frac{(v_1/v_2)X}{(1+(v_1/v_2)X)}\sim beta(\frac{v_1}{2},\frac{v_2}{2})$
\end{itemize}

\section*{Sufficient Statistics}
Contains all information about the parameters. 
A statistic $T(X)$ is sufficient for $\theta$ if the conditional distribution of the sample $X$ given the value of $T(X)$ does not depend on $\theta$.

\subsection*{Theorem}
If $p(x|\theta)$ is the joint pdf or pmf of $X$ and $q(T(x)|\theta)$ is the pdf or pmf of $T(X)$ then $T(X)$ is a sufficient statistic of $\theta$ if, for every $x$ in the sample space the ratio
$$\frac{p(x|\theta)}{q(T(x)|\theta)}$$
is constant as a function of $\theta$.
\begin{enumerate}
\item $P(x|p)=\prod\limits_{i=1}^nP(X=x)$
\item Find $q(T(x)|\mu)$
\item Find $\frac{p(x|\theta)}{q(T(x)|\theta)}$
\end{enumerate}

\subsection*{Factorization Theorem}
A statistic $T(X)$ is sufficient statistic for $\theta$ \emph{if and only if} there exist function $g(T(x)|\theta)$ and $h(x)$ such that, for all sample points $x$ and all parameter point $\theta$,
$$f(x|\theta)=g(T(x)|\theta) \cdot h(x)$$

\textbf{Example:} say we have $X_i$ is normally distributed and we want to prove the statistic $T=\sum_{i=1}^nX_i$ is sufficient for $\mu$. 
$f(x|\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}^{n/2}}exp\{-\frac{\sum_{i=1}^n(X_i-\mu)^2}{2\sigma^2}\}$
=$\frac{1}{\sqrt{2\pi\sigma^2}^{n/2}}exp\{-\frac{\sum_{i=1}^nX_i^2 -2\mu\sum_{i=1}^nX_i+n\mu^2}{2\sigma^2}\}$
=$\frac{1}{\sqrt{2\pi\sigma^2}^{n/2}}exp\{\frac{2\mu T-n\mu^2}{2\sigma^2}\} exp\{-\frac{\sum_{i=1}^nX_i^2}{2\sigma^2}\}$
Let: $g(T(X)|\mu)=(2\pi\sigma^2)^{n/2} exp\{\frac{2\mu T-n\mu^2}{2\sigma^2}\}$; $h(x)=exp\{-\frac{\sum_{i=1}^nX_i^2}{2\sigma^2}\}$
Follows the factorization theorem $f(x|\theta)=g(T(x)|\theta) \cdot h(x)$.

\section*{Complete Statistic}
Contains no irrelevant information about the parameters.
Let $f(T(x)|\theta)$ be a family of pdfs or pmfs for a statistic $T(X)$. The family of probability distributions is called \emph{complete} if $e_\theta g(T)=0$ for all $\theta$ implies $P_\theta(g(T)=0)=1$ for all $\theta$. Equivalently, $T(X)$ is called a \emph{Complete statistic}.\\
\textbf{Example 15.2}: Consider the statistic $T(X)-\sum X_i$ where $X_1\dots X_i\sim Bernoulli(p)$ Is it complete? We apply the definition. 
$E g(t)=\sum_{t=0}^n g(t) {n \choose t}p^t(1-p)^{n-t}$ = $(1-p)^n \sum_{t=0}^n g(t) {n \choose t} (\frac{p}{1-p})^t=0$
If $E g(t) = 0$ when $g(t)=0$ then $T(X)$ is a complete statistic. We end up with $\sum (g(t)) {n \choose t} (\frac{p}{1-p})^t=0$ This is a polynomial, so as $g(t)$ goes to zero, so does the entire function. It is Complete.

\section*{Likelihood function}
$f(x|\theta) \sim X = (X_1 \dots X_2)$ If $X=x$ is observed, the function of $\theta$ defined by
$$L(\theta|x)=f(x|\theta)$$
is a likelihood function. 

\subsection*{Likelihood principle}
Given points $x$ and $y$ $\exists C(x,y)$ where C is a constant such that
$$L(\theta|x)=C(x,y)L(\theta|y), \forall \theta$$
The likelihood principle says that if two sample points have proportional liklihoods then they contain equivalent information about $\theta$.

\subsection*{Point Estimation Method}
\begin{tabularx}{\textwidth/\NumCols}{| c | c | X |}
\hline
\textbf{Distribution} & \textbf{Parameters} & \textbf{Estimator} \\ \hline
\multirow{2}{*}{Normal} & $\mu$ & $\hat{\mu} = \bar{X} $ \\ \cline{2-3}
 & $\sigma^2$ & $\hat{\sigma}^2=S^2$ \\ \hline
Binomial & $p$ & $\hat{p}=\frac{1}{n}\sum_{i=1}^{n}X_i=\bar{X}, \{x_i\}$ are either 1 or 0, corresponding to the "success" and "failure" of the ith Bernoulli trial, respectively. \\ \hline
Poisson & $\lambda$ & $\hat{\lambda}=\frac{1}{n}\sum_{i=1}^{n}X_i = \bar{X}$ \\ \hline
\end{tabularx}

\section*{Estimators}

\subsection*{Method of Moment (MoM)}
$$\begin{cases}
\frac{1}{n}\sum\limits_{i=1}^{n}X_i = E(X^1_i) & First Moment \\
\frac{1}{n}\sum\limits_{i=1}^{n}X_i^2 = E(X^2_i) & Second Moment \\
\vdots \quad \vdots \quad \vdots & \vdots \\
\frac{1}{n}\sum\limits_{i=1}^{n}X_i^k = E(X^k_i) & k^{th} Moment \\
\end{cases}$$
You have n equations and n unknowns, solve for $\mu$, $\sigma$, etc.
\subsection*{Maximum Likelihood Estimation (MLE)}
\begin{itemize}
\item Write down likelihood function \\ $L(\theta|x)=\prod\limits_{i=1}^{n}P_{\theta}(X=x)$
\item Solve for theta\\ $\frac{\partial ln(L(\theta|x))}{\partial \theta_i}=0$
\item Verify the second derivative is less than zero\\ $\frac{\partial^2 ln(L(\theta|x))}{\partial \theta_i^2}<0$
\end{itemize}

\subsection*{Bayes Estimator}
$$\pi(\theta|x)=\frac{f(x|\theta)\pi(\theta)}{m(x)}$$
$$m(x)=\int f(x|\theta)\pi(\theta)d\theta$$
\textbf{Example}: Calculate the Posterior Distribution (i.e. the Bayes estimator) of $\lambda$. $X_1\dots X_n$ iid on Poisson, $\lambda \sim Gamma(\alpha, \beta)$.
$m(y) = \int_0^\infty f(y|\lambda)\pi(\lambda)d\lambda$ = $\int_0^\infty poisson(y|\lambda)Gamma(\lambda|\alpha,\beta)d\lambda$ = $\int_0^\infty\frac{e^{-n\lambda}(n\lambda)}{y!}\frac{1}{\Gamma{\alpha}\beta^\alpha}\lambda^{\alpha-1}e^{-\lambda/\beta}$=$\frac{n^y}{y!\Gamma(\alpha)\beta^\alpha}\Gamma(y+\alpha)(\frac{\beta}{n\beta+1})^{y+\alpha}$
$\pi(\lambda|y)=\frac{\lambda^{(y+\alpha)-1}e^{-\frac{\lambda}{\beta/(n\beta+1)}}}{\Gamma(y+\alpha)(\frac{\beta}{n\beta+1})^{y+\alpha}} \sim Gamma(y+\alpha, \frac{\beta}{n\beta+1})$
\subsection*{Evaluating Estimators}
\subsubsection*{Bias}
$$Bias(\hat{\theta})=E(\hat{\theta})-\theta$$

\begin{itemize}
\item Measures the \emph{accuracy} of the estimator
\item An estimator whose bias is zero is called \emph{unbiased}
\item An unbiased estimator may, nevertheless, fluctuate greatly from sample to sample
\end{itemize}

\subsubsection*{Var}
$$Var(\hat{\theta})=E{[\hat{\theta}-E(\hat{\theta})]^2}$$

\begin{itemize}
\item The lower the vaiance, the more \emph{precise} the estimator.
\item A low-variance estimator may be biased
\item Among the unbiased estimators, the one with the lowest variance should be chosen. "Best" - minimum variance
\end{itemize}

\subsubsection*{Mean Squared Error}
\begin{itemize}
\item To choose among all estimators (biased and unboased). Minimze a measure that combines both bias and variance.
\item A "good" estimator should have low bias and (accurate) AND low vairance (precise).
\end{itemize}
$$MSE(\hat{\theta})=E{[\hat{\theta}-\theta]^2}=Var(\hat{\theta})+[Bias(\hat{\theta})]^2$$
$$Bias(\hat{\theta})=E(\hat{\theta})-\theta$$
$$Var(\hat{\theta})=E{[\hat{\theta}-E(\hat{\theta})]^2}$$
\textbf{Example}: find the MSE of the estimators $\bar{X}$ and $S^2$ for the normal distribution in terms of $\mu$ and $\sigma$.
$MSE(\bar{X})=E(\bar{X}-\mu)^2 = Var(\bar{X})+[Bias(\bar{X})]^2$ Bias cancels to zero because $\bar{X}$ is an unbiased estimator so:
$MSE(\bar{X})=\frac{\sigma^2}{n}$\\
$MSE(S^2)=E(S^2-\sigma^2)^2=Var(S^2)+[Bias(S^2]^2$ Bias cancels to zero because $S^2$ is an unbiased estimator so:
$MSE(S^2)=Var(S^2)=\frac{2\sigma^4}{n-1}$
\subsection*{Uniform Minimum Variance Unbiased Estimator (UMVUE)}
An estimator $W*$ is a \emph{best unbiased} estimator of $\tau(\theta)$ if it statisfies $E_\theta W* = \tau(\theta), \forall \theta$ and for any other estimator $W$ with $E_\theta W = \tau(\theta)$ we have $Var_\theta W* \leq Var_\theta W$

\subsection*{Relating Sufficiency Stats to Unbiased Est.}
Given X and Y as RVs and the expectations exist, then
$$E(X)=E[E(X|Y)]$$
$$Var(X) = Var[E(X|Y)]+E[Var(X|Y)]$$

\subsection*{Rao-Blackwell theorem}
$\sum_{k=0}^\infty h(x) *g(x)$ where $g(x)$ is the distribution and $h(x)$ is some function. 
If you show that the function is zero only when $h(x)$ is zero then it passes the RB theorem. 

If you have complete, sufficient and unbiased and if the distribution passes this theorem then it is a UMVUE. 

\textbf{Example 20.2}: Binomial best unbiased estimator. For $X_1\dots X_i\sim Bernoulli(p)$ Define an estimator of P and show it is based on a sufficient and complete statistic. $\bar{X}=\frac{\sum X_i}{n} = \hat{p}$: is an unbiased estimator. Is it a UMVUE? Use Rao-Blackwell: $T=\sum X_i = Y \sim Binomial(n,p)$ $\theta=p$, $\tau(\theta)=p$, $\phi(T)$ is $\frac{T}{n}$, which is unbiased. T is a sufficient statistic for P, recall Example 15.2 T is a complete statistic. Therefore $\phi(T)$ is a UMVUE.

\section*{Hypothesis Testing}

\subsection*{Likelihood Ratio Test (LRT)}
Testing $H_0:\theta \in \Theta_0$ vs $H_1:\theta \in \Theta_0^C$ is 
$$\lambda(x)=\frac{sup_{\Theta_0} L(\theta|x)}{sup_{\Theta} L(\theta|x)}$$
Top part is the ratio between 2 maximum likelihood tests $\Theta=\Theta_0 \cup \Theta_0^C$ Bottom part is the global max. 
A LRT is any test that has a rejection region of the form $\{x:\lambda(x)\leq c\}$ where c is an number satisfying $0 \leq c \leq 1$.

\subsection*{Neyman-Pearson Lemma}
\begin{tabular}{ c  c | c | c }
\hline
\multirow{2}{*}{} &  & \multicolumn{2}{c}{Decision} \\

             &       &Accept $H_0$ & Reject $H_0$\\
\hline
      \multirow{2}{*}{Truth} &$H_0$            & \makecell{Correct decision \\ "Confidence" \\ $1-\alpha$}     & \makecell{Type I Error \\ "Significance Level" \\ $\alpha$}\\
\cline{3-4}
 & $H_1$           &   \makecell{Type II Error \\ "Failure to Detect" \\ $\beta$}   & \makecell{Correct decision \\ "Prob. of Detection" \\ $1-\beta$} \\
\hline
\end{tabular}

\begin{itemize}
\item $\alpha$ = P\{Type I error\} = P\{Reject $H_0$ when $H_0$ is true\} = P\{Reject $H_0|H_0$\}
\item $\beta$ = P\{Type II error\} = P\{Fail to reject $H_0$ when $H_1$ is true\} = P\{Fail to reject $H_0|H_1$\}
\item $\pi$ = $1-\beta$ = P\{Reject $H_0$ | $H_1$\} is prob. of detection or power of the test
\end{itemize}

\subsection*{Methods of Evaluating Tests}
\subsubsection*{Power Function}
Suppose $R$ denotes the rejection region for a test. For $\theta \in \Theta_0$ the test will make a mistake if $x \in R$. The probability of a Type I Error is $P_\theta(X\in R)$. The probability of a Type II Error is $P_\theta(X\in R^C)$. $P_\theta(X\in R) = 1-P_\theta(X\in R)$. We have:
$$P_\theta(X\in R)=\begin{cases}
Prob\quad Type\quad I \quad Error & \theta \in \Theta_0 \\
1-  Prob\quad Type\quad II \quad Error & \theta \in \Theta_0^C \\
\end{cases}$$
The \emph{Power Function} of a hypothesis test with rejection region $R$ is the function of $\theta$ defined by $\beta(\theta)=P_\theta(X\in R)$. The power function is your probability of Type II error.

\subsection*{Most Powerful Tests}
A test in class $\mathcal{C}$, with power function $\beta(\theta)$, is a \emph{uniformly most powerful (UMP) $\mathcal{C}$ class test} if $\beta(\theta) \geq \beta'(\theta)$ for every $\theta \in \Theta_0^C$ and every $\beta'(\theta)$ that is a powerful function of a test in class $\mathcal{C}$. 

\subsection*{Inference on MEAN of normal with Variance Known}
Use Z test\\
Confidence Intervals:
$$\mu_0:\frac{\bar{x}-\mu_0}{\sigma / \sqrt{n}} \sim N(0,1); \quad \bar{x}-Z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{x} + Z_{\alpha/2}\frac{\sigma}{\sqrt{n}}$$
$$\mu_0: \bar{x} + Z_\alpha \frac{\sigma}{\sqrt{n}} \geq \mu$$
$$\mu_0: \bar{x} - Z_\alpha \frac{\sigma}{\sqrt{n}} \leq \mu$$

Rejection Regions:
\begin{tabular}{| c | c | c | c |}
\hline
$H_0:\mu=\mu_0$ & $H_1$ &  & P Value\\ 
Reject $H_0$: & $\mu \neq \mu_0$ & if $\begin{vmatrix} \frac{\bar{x}-\mu_0}{\sigma / \sqrt{n}} \end{vmatrix} > Z(\alpha / 2)$ & $2(1-\Phi(z))$ \\ \hline
Reject $H_0$: & $\mu < \mu_0$ & if $\frac{\bar{x}-\mu_0}{\sigma / \sqrt{n}} < -Z(\alpha)$  & $\Phi(z)$\\ \hline
Reject $H_0$: & $\mu > \mu_0$ & if $\frac{\bar{x}-\mu_0}{\sigma / \sqrt{n}} > Z(\alpha)$  & $1-\Phi(z)$\\ \hline
\end {tabular}

\subsection*{Inference on MEAN of normal with Variance Unknown}
Use t test\\
Confidence Intervals:
$$\frac{\bar{x}-\mu}{s / \sqrt{n}} \sim t_{(n-1)}; \quad \bar{x}-t_{\alpha/2} \frac{s}{\sqrt{n}} \leq \mu \leq \bar{x} + t_{\alpha/2}\frac{s}{\sqrt{n}}$$
$$\mu_0: \bar{x} + t_\alpha \frac{s}{\sqrt{n}} \geq \mu$$
$$\mu_0: \bar{x} - t_\alpha \frac{s}{\sqrt{n}} \leq \mu$$
Rejection Regions:
\begin{tabular}{| c | c | c | l |}
\hline
$H_0:\mu=\mu_0$ & $H_1$ & & P Value \\ 
Reject $H_0$: & $\mu \neq \mu_0$ & if $\begin{vmatrix} \frac{\bar{x}-\mu_0}{s / \sqrt{n}} \end{vmatrix} > t(\alpha / 2, n-1)$ & $2(1-t_{n-1}(|\frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}}|)$ \\ \hline
Reject $H_0$: & $\mu < \mu_0$ & if $\frac{\bar{x}-\mu_0}{s / \sqrt{n}} < -t(\alpha, n-1)$ & $t_{n-1}(\frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}})$\\ \hline
Reject $H_0$: & $\mu > \mu_0$ & if $\frac{\bar{x}-\mu_0}{s / \sqrt{n}} > t(\alpha, n-1)$  & $1-t_{n-1}(\frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}})$\\ \hline
\end {tabular}

\subsection*{Significance Testing}
Significance is the area underneath the normal curve up to a certain point on the x axis. When testing for significance you want to find the point on the x axis, ($Z$), which equals the value of the significance from the standard normal CDF. When this $Z$ value is found then it can be compared with a calculated sigma calculation, which is based on the distribution being sampled. 


\section*{Misc.}
\begin{tabular}{| c | c | c |}
\hline
 & E & Var \\ \hline
$\bar{x}$ & $E(\bar{x})=\mu$ & $Var(\bar{x})=\frac{\sigma^2}{n}$ \\ \hline
$S^2$ & $E(S^2)=\sigma^2$ & $Var(S^2)=\frac{2 \sigma^4}{n-1}$ \\ \hline
\end{tabular}
$$\sigma^2 = \frac{1}{n} \sum (x_i-\bar{x})^2 = \frac{(n-1)s^2}{n}$$

When sampling with the distribution on the left you get the distribution on the right \\
\begin{tabular}{| c  c  c |}
\hline
Normal & $\rightarrow$ & Normal\\ \hline
Bernolli & $\rightarrow$ &  Binomial\\ \hline
Exponential & $\rightarrow$ & Gamma\\ \hline
\end{tabular}

\scriptsize
\bibliographystyle{unsrt} 
\bibliography{course-references}
\end{multicols}
\end{document}

