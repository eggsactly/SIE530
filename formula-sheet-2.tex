\documentclass[8pt,landscape]{extarticle}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage[hidelinks]{hyperref}
\usepackage{tabularx}
\newcommand\NumCols{3}
\pdfinfo{
  /Title (formula-sheet-2.pdf)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (Garrett Weaver)
  /Subject (SIE 530)
  /Keywords (statistics, engineering, masters, Arizona, Liu)}

% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
    {\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
    }

% Turn off header and footer
\pagestyle{empty}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

%My Environments
\newtheorem{example}[section]{Example}
% -----------------------------------------------------------------------

\begin{document}

\raggedright
%\footnotesize\usepackage{amsmath}
\begin{multicols}{\NumCols}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\underline{SIE 530 Formula Sheet 2}} \\
\end{center}

\section{Snedecor's \emph{F}-Distribution}
Consider these two distributions \\
$X_1, \dots X_n \sim N(\mu_X, \sigma_X^2)$ \\
$Y_1, \dots Y_m \sim N(\mu_Y, \sigma_Y^2)$ \\
RV $F$ is the ratio of two independent $\chi^2$ variables divided by their respective degree of freedom:
$$F=(S_X^2/\sigma_X^2)/(S_Y^2/\sigma_Y^2)$$
has Snedecor's $F$-Distribution with $v_1=n-1$ and $v_2=m-1$ degrees of freedom (i.e. $F \sim F_{v_1,v_2}$)

$$f_F(x|v)=\frac{\Gamma(\frac{v_1+v_2}{2})}{\Gamma(\frac{v_1}{2})\Gamma(\frac{v_2}{2})}
\begin{pmatrix} \frac{v_1}{v_2} \end{pmatrix}^{\frac{v_1}{2}}
\frac{x^{\frac{(v_1-2)}{2}}}{\begin{pmatrix}1+\begin{pmatrix}\frac{v_1}{v_2}\end{pmatrix}x\end{pmatrix}^{\frac{(v_1+v_2)}{2}}}$$
$0\leq\infty, v_1,v_2=1\dots$

\subsection{$F$-Distribution Properties}
\begin{itemize}
\item $EX=\frac{v_2}{v_2-2}, v_2 > 2$
\item $VarX = 2(\frac{v_2}{v_2-2})^2\frac{v_1+v_2-2}{v_1(v_2-4)}, v_2>4$
\item If $X\sim F_{v_1,v_2}$ then $1/X \sim F_{v_2,v_1}$
\item If $X \sim t_q$ then $X^2 \sim F_{1,q}$
\item If $X \sim F{v_1,v_2}$ then \\ $\frac{(v_1/v_2)X}{(1+(v_1/v_2)X)}\sim beta(\frac{v_1}{2},\frac{v_2}{2})$
\end{itemize}

\section{Sufficient Statistics}
Contains all information about the parameters. 
A statistic $T(X)$ is sufficient for $\theta$ if the conditional distribution of the sample $X$ given the value of $T(X)$ does not depend on $\theta$.

\subsection{Theorem}
If $p(x|\theta)$ is the joint pdf or pmf of $X$ and $q(T(x)|\theta)$ is the pdf or pmf of $T(X)$ then $T(X)$ is a sufficient statistic of $\theta$ if, for every $x$ in the sample space the ratio
$$\frac{p(x|\theta)}{q(T(x)|\theta)}$$
is constant as a function of $\theta$.
\begin{enumerate}
\item $P(x|p)=\prod\limits_{i=1}^nP(X=x)$
\item Find $q(T(x)|\mu)$
\item Find $\frac{p(x|\theta)}{q(T(x)|\theta)}$
\end{enumerate}

\subsection{Factorization Theorem}
A statistic $T(X)$ is sufficient statistic for $\theta$ \emph{if and only if} there exist function $g(T(x)|\theta)$ and $h(x)$ such that, for all sample points $x$ and all parameter point $\theta$,
$$f(x|\theta)=g(T(x)|\theta) \cdot h(x)$$

\section{Complete Statistic}
Contains no irrelevant informations about the parameters.
Let $f(T(x)|\theta)$ be a family of pdfs or pmfs for a statistic $T(X)$. The family of probability distributions is called \emph{complete} if $e_\theta g(T)=0$ for all $\theta$ implies $P_\theta(g(T)=0)=1$ for all $\theta$. Equivalently, $T(X)$ is called a \emph{Complete statistic}.

\section{Likelihood function}
$f(x|\theta) \sim X = (X_1 \dots X_2)$ If $X=x$ is observed, the function of $\theta$ defined by
$$L(\theta|x)=f(x|\theta)$$
is a likelihood function. 

\subsection{Likelihood principle}
Given points $x$ and $y$ $\exists C(x,y)$ where C is a constant such that
$$L(\theta|x)=C(x,y)L(\theta|y), \forall \theta$$

\section*{Estimators}

\subsection*{Method of Moment (MoM)}
$$\begin{cases}
\frac{1}{n}\sum\limits_{i=1}^{n}X_i = E(X^1_i) & First Moment \\
\frac{1}{n}\sum\limits_{i=1}^{n}X_i^2 = E(X^2_i) & Second Moment \\
\vdots \quad \vdots \quad \vdots & \vdots \\
\frac{1}{n}\sum\limits_{i=1}^{n}X_i^k = E(X^k_i) & k^{th} Moment \\
\end{cases}$$
You have n equations and n unknowns, solve for $\mu$, $\sigma$, etc.
\subsection*{Maximum Likelihood Estimation (MLE)}
\begin{itemize}
\item Write down likelihood function \\ $L(\theta|x)=\prod\limits_{i=1}^{n}P_{\theta}(X=x)$
\item Solve for theta\\ $\frac{\partial ln(L(\theta|x))}{\partial \theta_i}=0$
\item Verify the second derivative is less than zero\\ $\frac{\partial^2 ln(L(\theta|x))}{\partial \theta_i^2}<0$
\end{itemize}

\subsection*{Bayes Estimator}
$$\pi(\theta|x)=\frac{f(x|\theta)\pi(\theta)}{m(x)}$$
$$m(x)=\int f(x|\theta)\pi(\theta)d\theta$$

\subsection*{Evaluating Estimators}
\subsubsection*{Bias}
$$Bias(\hat{\theta})=E(\hat{\theta})-\theta$$

\begin{itemize}
\item Measures the \emph{accuracy} of the estimator
\item An estimator whose bias is zero is called \emph{unbiased}
\item An unbiased estimator may, nevertheless, fluctuate greatly from sample to sample
\end{itemize}

\subsubsection*{Var}
$$Var(\hat{\theta})=E{[\hat{\theta}-E(\hat{\theta})]^2}$$

\begin{itemize}
\item The lower the vaiance, the more \emph{precise} the estimator.
\item A low-variance estimator may be biased
\item Among the unbiased estimators, the one with the lowest variance should be chosen. "Best" - minimum variance
\end{itemize}

\subsubsection*{Mean Squared Error}
\begin{itemize}
\item To choose among all estimators (biased and unboased). Minimze a measure that combines both bias and variance.
\item A "good" estimator should have low bias and (accurate) AND low vairance (precise).
\end{itemize}
$$MSE(\hat{\theta})=E{[\hat{\theta}-\theta]^2}=Var(\hat{\theta})+[Bias(\hat{\theta})]^2$$
$$Bias(\hat{\theta})=E(\hat{\theta})-\theta$$
$$Var(\hat{\theta})=E{[\hat{\theta}-E(\hat{\theta})]^2}$$


\scriptsize
\bibliographystyle{unsrt} 
\bibliography{course-references}
\end{multicols}
\end{document}

