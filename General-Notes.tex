\documentclass[12pt]{article}
\usepackage{lingmacros}
\usepackage{tree-dvips}
\usepackage{amsmath}
\usepackage{accents}
\usepackage[hidelinks]{hyperref}
\usepackage{multirow, tabularx} % in the preamble

\newcommand{\ubar}[1]{\underaccent{\bar}{#1}}

\setlength\parindent{0pt}

\begin{document}

\title{SIE 530 Comprehensive Notes}
\date{November 6, 2017\\Version: 3}
\maketitle
{\centering

Fork on GitHub: \url{https://github.com/eggsactly/SIE530}\par
}
\section*{Definitions}

\begin{itemize}

\item Disjoint: Event $A$ and event $B$ are disjoint, or mutually exclusive, if $A$ and $B$ have no outcome in common $A \cap B = \emptyset \leftrightarrow$ $A$ and $B$ are disjoint events. \cite[p.18]{classnotes.1}

\item Event: A collection of elements contained in the \emph{sample space} \cite[p.16]{classnotes.1}

\item Exhaustive: Event $A$ and event $B$ are exhaustive if the union is $S$. If $A \cap B = \emptyset \leftrightarrow$ $A$ and $B$ are exhaustive events. \cite[p.18]{classnotes.1}

\item Independent: Two events $A$ and $B$ are said to be independent if $P(A|B)=P(A)$ \cite[p.8]{classnotes.3}

\item Median: A value such that at least 50\% of the data values are at of below this value and at least 50\% of the data values are at or above this value. (Not sensitive to the outlier). \cite[p.9]{classnotes.1}

\item Nondecreasing: A function f(x) is said to be \emph{nondecreasing} on an interval $I$ if $f(b)\geq f(a)$ for all $b>a$, where $a,b \in I$. \cite{wolfram.nondecreasing}

\item Population: A finite well defined group, which can be enumerated in theory. \cite[p.9]{classnotes.1}

\item Random Experiment: an action or process whos outcome is uncertain \cite[p.16]{classnotes.1}

\item Random Variable: A variable with unknown numerical value that can take on or represent any possible element from a \emph{sample space} \cite[p.10]{classnotes.3}

\item Right Continuous: A function is \emph{Right Continuous} when a point as approached from the right \cite[p.13]{classnotes.3}

\item Sample: A subset of a population obtained through a process (possibly random selection). \cite[p.9]{classnotes.1}

\item Sample Space ($S$): A collection of all possible outcomes of a \emph{Random Experiment} \cite[p.16]{classnotes.1}

\end{itemize}

\newpage

\section*{Probability Theory}

Central Tendency: sample average/mean \cite[p.14]{classnotes.1}.
$$\bar{x}=\frac{\sum\limits_{i=1}^{n} x_i}{n}$$

Scatter/dispersion: \emph{sample variance} or \emph{sample standard deviation} \cite[p.14]{classnotes.1}.

$$\hat{\sigma}=S=\sqrt{\frac{\sum\limits_{i=1}^{n}(x_i-\bar{x})^2}{n-1}}$$

\subsection*{Set Theory}
\begin{itemize}

\item $s\in S$: The outcome $s$ belongs to the sampel space $S$ \cite[p.17]{classnotes.1}

\item $\emptyset=\{\}$: The empty set, the set of no elements. Defines the set of elements of an impossible event \cite[p.17]{classnotes.1}

\item Union: The union of event $A$ and event $B$ denonted $A \cup B$, is the collection (or set) of elements that belong to \emph{either} $A$ or $B$ or both \cite[p.17]{classnotes.1}

\item Intersection: The intersection of event $A$ and event $B$ denoted $A \cap B$ is the collection (or set) of elements that belong to $A$ and $B$ \cite[p.17]{classnotes.1}

\item Complementation: collection of elements that do not belong to $A$. $A^c=\{x:x\notin A\}$ \cite[p.17]{classnotes.1}

\item $A\subset B$: The event course-references.bib$A$ is contained in event $B$, $A\subset B$, if every element of $A$ also belonegs to $B$ \cite[p.17]{classnotes.1}

\end{itemize}

\subsection*{Properties of Sets \cite[p.19]{classnotes.1}}

\begin{itemize}

\item Commutativity: $A \cup B = B \cup A; A \cap B = B \cap A$

\item Associativity: $A\cup (B \cup C)=(A \cup B) \cup C; A \cap (B \cap C) = (A \cap B) \cap C$

\item Distributive Laws: $A \cap(B \cup C) = (A \cap B) \cup ( A \cap C)$ $ A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$

\item DeMorgan's Laws:
$(A \cup B)^c=A^c \cap B^c$
$(A \cap B)^c=A^c\cup B^c$

\end{itemize}

\subsection*{Definition of Probability \cite[p.4]{classnotes.2}}
[P \(\) : \{set of all possible events\} $\rightarrow [0,1]$]
\begin{itemize}

\item Axiom 1: For any event $A, P(A)\geq 0$ (nonnegatove).
\item Axiom 2: $P(S)=1$
\item Axiom 3: For any sequence of disjoint sets $A_1,A_2,\dots,A_n, P(A_1 \cup A_2 \cup \dots A_n)=\sum\limits_{i=1}^nP(A_i)$, where n is the total number of disjoint sets in the sequence.

\end {itemize}

\subsection*{Properties and Additive Laws \cite[p.5]{classnotes.2}}
\begin{itemize}
\item $P(A)=1-P(A^c);$
\item $P(\emptyset)=0$
\item $P(S)=1$
\item if $A$ and $B$ are disjoint $P(A\cup B)=0$
\item $A\subset B \rightarrow P(A)\leq (B)$
\item $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
\end{itemize}

Bonferroni's inequality \cite[p.6]{classnotes.2}
$$P(A\cap B) \geq P(A) + P(B) - 1$$

Boole's inequality \cite[p.5]{classnotes.2}: for any sets $A_1, A_2, \dots \subset S$
$$P(\bigcup\limits_{i=1}^\infty)\leq \sum\limits_{i=1}^\infty P(A_i)$$

\subsection*{Counting Techniquies}

\begin{itemize}
\item Permutations: Select k objects from a total n without replacement and order matters, what is the number of permutations? \cite[p.9]{classnotes.2}:
$$P_{k,n} = \frac{n!}{(n-k)!}$$
\item Combinations: Select k objects from a total of n without replacement and order does not matter, what is the number of combinations? \cite[p.10]{classnotes.2}
$$C_{k,n}=
\begin{pmatrix}
n \\
k
\end{pmatrix}
=\frac{n!}{k!(n-k)!}
$$
\end{itemize}

\subsection*{Conditional Probability}
Probability of $A$ such that $B$ has occured \cite[p.12]{classnotes.2}:
$$P(A|B)=\frac{P(A \cap B)}{P(B)}$$ 
$$P(A|B)P(B)=P(B|A)P(A)$$
$$P(A_1|B)+P(A_2|B)+\dots P(A_k|B)=1$$
Law of total probability:
$$\sum\limits_{i=1}^k P(B|A_i)P(A_i)=P(B)$$

\subsection*{Bayes Theorem}
Let the events $A_1,A_2,\dots A_k$ be \emph{disjoint} and \emph{exhaustive} events in the sample space $S$, such that $P(A_i)>0$ and let $B$ be an event such that $P(B)>0$, then, \cite[p.7]{classnotes.3}:
$$P(A_i|B)=\frac{P(B|A_i)P(A_i)}{\sum_{j=1}^k P(B|A_i)P(A_j)}$$
$$P(A_i|B)=\frac{P(A_i \cap B)}{P(B)} = \frac{cond. \quad prob.}{law \quad of \quad total \quad prob.}$$

\subsection*{Discrete Random Variables}
\emph{Probability mass function} (pmf) of a discrete \emph{Random Variable} is given by \cite[p.11]{classnotes.3}:
$$f_X(x)=P(X=x)$$
and satisfies the following properties
\begin{enumerate}
\item $f_X(x)\geq 0; \quad \forall x$
\item $\sum\limits_{i=1}^{\infty}f(x_i)=1$
\end{enumerate}

\emph{Cumulative Distribution Function} (cdf) of a discrete \emph{Random Variable} \cite[p.13]{classnotes.3}:
$$F_X(x)=P(X\leq x)=\sum\limits_{i=1}^{x}f_X(i); \quad \forall x$$

Satisfies the following properties
\begin{enumerate}
\item $\lim\limits_{x\to-\infty}F(x)=0$ and $\lim\limits_{x\to\infty}F(x)=1$
\item $F(x)$ is a \emph{nondecreasing} function of $x$
\item $F(x)$ is \emph{right-continuous} for every number $x_0, \quad \lim_{x\downarrow x_0}F(x)=F(x_0)$
\end{enumerate}

\subsection*{Continuous Random Variables}
The \emph{probability density function} (pdf) of a continuous \emph{Random Variable} is given by \cite[p.4]{classnotes.4}:
$$\int_{x_1}^{x_2}f_X(x)dx=P(x_1\leq X\leq x_2), \quad \forall x_1,x_2$$
And satisfies the following properties:
\begin{enumerate}
\item $f(x)\geq 0 \quad \forall x$
\item $\int_{-\infty}^{\infty}f(x)dx=1$
\end{enumerate}

The \emph{Cumulative Distribution Function}(cdf) for a continuous \emph{Random Variable} is \cite[p.5]{classnotes.4}:
$$F_X(x)=P(X \leq x) = \int_{-\infty}^{x}f_X(u)du, \quad \forall x$$
And satisifes the following property:
\begin{enumerate}
\item $\frac{\partial F_X(x)}{\partial x}=f_X(x)$ provided that $F'$ exists and $X$ is a continuous \emph{Random Variable}
\end{enumerate}

\section*{Properties of a Random Sample}
\subsection*{Lemma 5.3.2 (Facts about chi squared random variables)\cite[p. 219]{StatisticalInference}}
We use the notation $\chi_p^2$ to denote a chi squared random variable with p degrees of freedom. 
\begin{enumerate}
\item If $Z$ is a $n(0, 1)$ \emph{random variable}, then $Z^2\sim\chi_1^2$; that is, the square of a standard normal random variable is a chi squared random variable
\item If $\chi_1,\dots\chi_n$ are independent and $\chi_i\sim\chi_{p_i}^2$, then $\chi_1+\dots+\chi_n\sim\chi_{p_1+\dots+p_n}^2$; that is, independent chi squared variables add to a chi squared variable, and the degrees of freedom also add.
\end{enumerate}

\section*{Sufficiency}
\subsection*{Sufficient Principle \cite[p. 8]{classnotes.14}}
Theorem: $T(X)$ is a sufficient statistic for $\theta$ if for every $x$ in the same space, the ratio $\frac{p(x|\theta)}{q(T(x)|\theta)}$ is a constant function of $\theta$.
\begin{enumerate}
\item $P(x|p)=\prod\limits_{i=1}^nP(X=x)$
\item Find $q(T(x)|\mu)$
\item Find $\frac{p(x|\theta)}{q(T(x)|\theta)}$
\end{enumerate}

\subsection*{Factorization Theorem \cite[p. 4]{classnotes.15}}
A statistic $T(X)$ is sufficient statistic for $\theta$ if and only if there esists a function $g(T(X)\theta)$ and $h(x)$ sch that, for all sample points $x$ and all parameter point $\theta$, 
$$f(x|\theta)=g(T(X)|\theta)h(X)$$

\section*{Complete Statistic}
Contains no irrelevant information about the parameters.
Let $f(T(x)|\theta)$ be a family of pdfs or pmfs for a statistic $T(X)$. The family of probability distributions is called \emph{complete} if $e_\theta g(T)=0$ for all $\theta$ implies $P_\theta(g(T)=0)=1$ for all $\theta$. Equivalently, $T(X)$ is called a \emph{Complete statistic}.

\section*{Likelihood function}
$f(x|\theta) \sim X = (X_1 \dots X_2)$ If $X=x$ is observed, the function of $\theta$ defined by
$$L(\theta|x)=f(x|\theta)$$
is a likelihood function. 

\subsection*{Likelihood principle}
Given points $x$ and $y$ $\exists C(x,y)$ where C is a constant such that
$$L(\theta|x)=C(x,y)L(\theta|y), \forall \theta$$
The likelihood principle says that if two sample points have proportional liklihoods then they contain equivalent information about $\theta$ \cite[p. 291]{StatisticalInference}. 


\section*{Estimators}

\subsection*{Method of Moment (MoM)}
$$\begin{cases}
\frac{1}{n}\sum\limits_{i=1}^{n}X_i = E(X^1_i) & First Moment \\
\frac{1}{n}\sum\limits_{i=1}^{n}X_i^2 = E(X^2_i) & Second Moment \\
\vdots \quad \vdots \quad \vdots & \vdots \\
\frac{1}{n}\sum\limits_{i=1}^{n}X_i^k = E(X^k_i) & k^{th} Moment \\
\end{cases}$$
You have n equations and n unknowns, solve for $\mu$, $\sigma$, etc.
\subsection*{Maximum Likelihood Estimation (MLE)}
\begin{itemize}
\item Write down likelihood function \\ $L(\theta|x)=\prod\limits_{i=1}^{n}P_{\theta}(X=x)$
\item Solve for theta\\ $\frac{\partial ln(L(\theta|x))}{\partial \theta_i}=0$
\item Verify the second derivative is less than zero\\ $\frac{\partial^2 ln(L(\theta|x))}{\partial \theta_i^2}<0$
\end{itemize}

\subsection*{Bayes Estimator}
$$\pi(\theta|x)=\frac{f(x|\theta)\pi(\theta)}{m(x)}$$
$$m(x)=\int f(x|\theta)\pi(\theta)d\theta$$

\subsection*{Evaluating Estimators \cite{classnotes.18}}
\subsubsection*{Bias}
$$Bias(\hat{\theta})=E(\hat{\theta})-\theta$$

\begin{itemize}
\item Measures the \emph{accuracy} of the estimator
\item An estimator whose bias is zero is called \emph{unbiased}
\item An unbiased estimator may, nevertheless, fluctuate greatly from sample to sample
\end{itemize}

\subsubsection*{Var}
$$Var(\hat{\theta})=E{[\hat{\theta}-E(\hat{\theta})]^2}$$

\begin{itemize}
\item The lower the variance, the more \emph{precise} the estimator.
\item A low-variance estimator may be biased
\item Among the unbiased estimators, the one with the lowest variance should be chosen. "Best" - minimum variance
\end{itemize}

\subsubsection*{Mean Squared Error}
\begin{itemize}
\item To choose among all estimators (biased and unboased). Minimze a measure that combines both bias and variance.
\item A "good" estimator should have low bias and (accurate) AND low vairance (precise).
\end{itemize}
$$MSE(\hat{\theta})=E{[\hat{\theta}-\theta]^2}=Var(\hat{\theta})+[Bias(\hat{\theta})]^2$$
$$Bias(\hat{\theta})=E(\hat{\theta})-\theta$$
$$Var(\hat{\theta})=E{[\hat{\theta}-E(\hat{\theta})]^2}$$


\section*{Hypothesis Testing}

\subsection*{Likelihood Ratio Test (LRT)}
Testing $H_0:\theta \in \Theta_0$ vs $H_1:\theta \in \Theta_0^C$ is 
$$\lambda(x)=\frac{sup_{\Theta_0} L(\theta|x)}{sup_{\Theta} L(\theta|x)}$$
Top part is the ratio between 2 maximum likelihood tests $\Theta=\Theta_0 \cup \Theta_0^C$ Bottom part is the global max. 
A LRT is any test that has a rejection region of the form $\{x:\lambda(x)\leq c\}$ where c is an number satisfying $0 \leq c \leq 1$.

\subsection*{Methods of Evaluating Tests}
\begin{tabular}{ c  c | c | c }
\hline
\multirow{2}{*}{} &  & \multicolumn{2}{c}{Decision} \\

             &       &Accept $H_0$ & Reject $H_0$\\
\hline
      \multirow{2}{*}{Truth} &$H_0$            & Correct decision     & Type I Error\\
\cline{3-4}
 & $H_1$           &   Type II Error   & Correct decision\\
\hline
\end{tabular}

\subsection*{Inference on MEAN of normal with Variance Known}
Use Z test
$$\frac{\bar{x}-\mu_0}{\sigma / \sqrt{n}} \sim N(0,1); \quad \bar{x}-Z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{x} + Z_{\alpha/2}\frac{\sigma}{\sqrt{2}}$$
\begin{tabular}{| c | c | c |}
\hline
$H_0:\mu=\mu_0$ & $H_1$ & \\ 
Reject $H_0$: & $\mu \neq \mu_0$ & if $\begin{vmatrix} \frac{\bar{x}-\mu_0}{\sigma / \sqrt{n}} \end{vmatrix} > Z(\alpha / 2)$  \\ \hline
Reject $H_0$: & $\mu < \mu_0$ & if $\frac{\bar{x}-\mu_0}{\sigma / \sqrt{n}} < -Z(\alpha)$  \\ \hline
Reject $H_0$: & $\mu > \mu_0$ & if $\frac{\bar{x}-\mu_0}{\sigma / \sqrt{n}} > Z(\alpha)$  \\ \hline
\end {tabular}

\subsection*{Inference on MEAN of normal with Variance Unknown}
Use t test
$$\frac{\bar{x}-\mu}{s / \sqrt{n}} \sim t_{(n-1)}; \quad \bar{x}-t_{\alpha/2} \frac{s}{\sqrt{n}} \leq \mu \leq \bar{x} + t_{\alpha/2}\frac{s}{\sqrt{n}}$$
\begin{tabular}{| c | c | c |}
\hline
$H_0:\mu=\mu_0$ & $H_1$ & \\ 
Reject $H_0$: & $\mu \neq \mu_0$ & if $\begin{vmatrix} \frac{\bar{x}-\mu_0}{s / \sqrt{n}} \end{vmatrix} > t(\alpha / 2, n-1)$  \\ \hline
Reject $H_0$: & $\mu < \mu_0$ & if $\frac{\bar{x}-\mu_0}{s / \sqrt{n}} < -t(\alpha, n-1)$  \\ \hline
Reject $H_0$: & $\mu > \mu_0$ & if $\frac{\bar{x}-\mu_0}{s / \sqrt{n}} > t(\alpha, n-1)$  \\ \hline
\end {tabular}








\newpage
\section*{Table of Common Distributions}

\subsection*{Discrete Distributions}
\subsubsection*{Bernoulli(p) \cite[p. 621]{StatisticalInference}}
\begin{tabularx}{\textwidth}{ l X }
\emph{pmf} & $P(X=x|p)=p^x(1-p)^{1-x}; \quad x=0, 1; \quad 0 \leq p \leq 1$ \\
\emph{mean} & $EX=p$ \\
\emph{variance} & $Var X =p(1-p)$ \\
\emph{mgf} & $M_X(t)=(1-p)+pe^t$ \\
\emph{Notes} & Special case of the Binomial distribution where only a single experiment is done.\\
\end{tabularx}

\subsubsection*{Binomial(n, p) \cite[p. 621]{StatisticalInference}}
\begin{tabularx}{\textwidth}{ l X }
\emph{pmf} & $P(X=x|n, p)=\begin{pmatrix}
n \\
x
\end{pmatrix}p^x(1-p)^{n-x}; \quad x=0, 1, 2, \dots, n; \quad 0 \leq p \leq 1$ \\
\emph{mean} & $EX=np$ \\
\emph{variance} & $Var X =np(1-p)$ \\
\emph{mgf} & $M_X(t)=[pe^t + (1-p)]^n$ \\
\emph{Notes} & None\\
\end{tabularx}

\subsubsection*{Hypergeometric \cite[p. 622]{StatisticalInference}}
\begin{tabularx}{\textwidth}{ l X }
\emph{pmf} & $P(X=x|N, M, K)=\frac{
\begin{pmatrix}
M \\
x
\end{pmatrix}
\begin{pmatrix}
N - M \\
k - x
\end{pmatrix}
}{
\begin{pmatrix}
N \\
K
\end{pmatrix}}; \quad x=0, 1, 2, \dots, K; \quad M - (N - K) \leq x \leq M; \quad N, M, K \geq 0$ \\
\emph{mean} & $EX=\frac{KM}{N}$ \\
\emph{variance} & $Var X = \frac{KM}{N} \frac{(N-M)(N-K)}{N(N-1)}$ \\
\emph{mgf} & None \\
\emph{Notes} & If $K \ll M$ and $N$, the range $x=0, 1, 2, \dots, K$ will be appropriate.\\
\end{tabularx}

\subsubsection*{Poisson($\lambda$) \cite[p. 622]{StatisticalInference}}
\begin{tabularx}{\textwidth}{ l X }
\emph{pmf} & $P(X=x|\lambda)=\frac{e^{-\lambda}\lambda^x}{x!}; \quad x=0, 1, \dots; \quad 0 \leq \lambda <\infty$ \\
\emph{mean} & $EX=\lambda$ \\
\emph{variance} & $Var X =\lambda$ \\
\emph{mgf} & $M_X(t)=e^{\lambda(e^t-1)}$ \\
\emph{Notes} & None\\
\end{tabularx}

\subsection*{Continuous Distributions}
\subsubsection*{Chi squared(p)\cite[p. 623]{StatisticalInference}}
\begin{tabularx}{\textwidth}{ l X }
\emph{pmf} & $f(x|p)=\frac{1}{\Gamma(p/2)2^{p/2}}x^{(p/2)-1}e^{-x/2}; \quad 0 \leq x < \infty; \quad p=1, 2, \dots$ \\
\emph{mean} & $EX = p$ \\
\emph{variance} & $Var X = 2p$ \\
\emph{mgf} & $M_X(t)=\begin{pmatrix} \frac{1}{1-2t} \end{pmatrix}^{p/2}, \quad t < \frac{1}{2}$ \\
\emph{Notes} & Special case of the gamma distribution.\\
\end{tabularx}

\subsubsection*{Exponential($\beta$)\cite[p. 624]{StatisticalInference}}
\begin{tabularx}{\textwidth}{ l X }
\emph{pmf} & $f(x|p)=\frac{1}{\beta}e^{-x/\beta}; \quad 0 \leq x < \infty; \quad \beta>0$ \\
\emph{mean} & $EX = \beta$ \\
\emph{variance} & $Var X = \beta^2$ \\
\emph{mgf} & $M_X(t)= \frac{1}{1-\beta t}, \quad t < \frac{1}{\beta}$ \\
\emph{Notes} & Special case of the gamma distribution. Has the \emph{memoryless} property. Has many special cases: $Y=X^{1/\gamma}$ is \emph{Weibull}, $Y=\sqrt{2X/\beta}$ is \emph{Rayleigh}, $Y=\alpha - \gamma log(X/\beta)$ \emph{Gumbel}\\
\end{tabularx}

\subsubsection*{Gamma($x|\alpha, \beta$)\cite[p. 624]{StatisticalInference}}
\begin{tabularx}{\textwidth}{ l X }
\emph{pmf} & $f(x|\alpha, \beta)=\frac{1}{\Gamma(\alpha)\beta^{\alpha}} x^{\alpha - 1}e^{-x/\beta}; \quad 0 \leq x < \infty; \quad \alpha\beta>0$ \\
\emph{mean} & $EX = \alpha\beta$ \\
\emph{variance} & $Var X = \alpha\beta^2$ \\
\emph{mgf} & $M_X(t)= \begin{pmatrix}\frac{1}{1-\beta t}\end{pmatrix}^{\alpha}, \quad t < \frac{1}{\beta}$ \\
\emph{Notes} & Some special cases are exponential ($\alpha = 1$) and shi squared ($\alpha = p/2, \beta = 2$). If $\alpha = \frac{3}{2}$, $Y=\sqrt{X/\beta}$ is \emph{Maxwell}, $Y=1/X$ has the \emph{inverted gama distribution}. Can also be related to the Poisson.\\
\end{tabularx}

\subsubsection*{Normal($\mu, \sigma^2$)\cite[p. 625]{StatisticalInference}}
\begin{tabularx}{\textwidth}{ l X }
\emph{pmf} & $f(x|\mu, \sigma^2)=\frac{1}{\sqrt{2\pi \sigma}}e^{-(x-\mu)^2/(2\sigma^2)}; \quad -\infty < x < \infty; \quad -\infty < \mu < \infty$ \\
\emph{mean} & $EX = \mu$ \\
\emph{variance} & $Var X = \sigma^2$ \\
\emph{mgf} & $M_X(t)= e^{\mu t + \sigma^2t^2/2}$ \\
\emph{Notes} & Sometimes called the \emph{Gaussian} distribution.\\
\end{tabularx}

\subsubsection*{Uniform($a, b$)\cite[p. 626]{StatisticalInference}}
\begin{tabularx}{\textwidth}{ l X }
\emph{pmf} & $f(x|a, b)=\frac{1}{b-a}, \quad a \leq x \leq b$ \\
\emph{mean} & $EX = \frac{b+a}{2}$ \\
\emph{variance} & $Var X = \frac{(b-a)^2}{12}$ \\
\emph{mgf} & $M_X(t)=\frac{e^{bt}-e^{at}}{(b-a)t}$ \\
\emph{Notes} & If $a=0$ and $b=1$, this is a special case of the beta $(\alpha = \beta = 1)$.\\
\end{tabularx}



\newpage
\bibliographystyle{unsrt}
\bibliography{course-references}

\end{document}
